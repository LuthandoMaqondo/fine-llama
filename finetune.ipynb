{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import accelerate\n",
    "from datasets import Dataset, load_dataset\n",
    "from huggingface_hub import login, notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook_login(new_session=True, write_permission=True)\n",
    "# login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoTrain version: 0.8.7\n"
     ]
    }
   ],
   "source": [
    "from autotrain import __version__\n",
    "print(f'AutoTrain version: {__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 3\n",
    "batch_size = 4\n",
    "block_size = 1024\n",
    "trainer = \"sft\"\n",
    "warmup_ratio = 0.03\n",
    "weight_decay = 0.\n",
    "gradient_accumulation = 4\n",
    "lora_r = 16\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-24 21:22:47\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m344\u001b[0m - \u001b[1mRunning LLM\u001b[0m\n",
      "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-07-24 21:22:47\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m180\u001b[0m - \u001b[33m\u001b[1mParameters supplied but not used: deploy, version, train, backend, func, config, inference\u001b[0m\n",
      "Saving the dataset (1/1 shards): 100%|████| 1/1 [00:00<00:00, 537.94 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|████| 1/1 [00:00<00:00, 589.34 examples/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-24 21:22:47\u001b[0m | \u001b[36mautotrain.backends.local\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mStarting local training...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-24 21:22:47\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m400\u001b[0m - \u001b[1m['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'no', '-m', 'autotrain.trainers.clm', '--training_config', 'fine-llama-2/training_params.json']\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-24 21:22:47\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1m{'model': 'llama/Llama-2-7b-hf/', 'project_name': 'fine-llama-2', 'data_path': 'fine-llama-2/autotrain-data', 'train_split': 'train', 'valid_split': None, 'add_eos_token': False, 'block_size': 1024, 'model_max_length': 1024, 'padding': None, 'trainer': 'default', 'use_flash_attention_2': False, 'log': 'wandb', 'disable_gradient_checkpointing': False, 'logging_steps': 10, 'eval_strategy': 'epoch', 'save_total_limit': 1, 'auto_find_batch_size': False, 'mixed_precision': None, 'lr': 2e-05, 'epochs': 3, 'batch_size': 4, 'warmup_ratio': 0.03, 'gradient_accumulation': 4, 'optimizer': 'adamw_torch', 'scheduler': 'linear', 'weight_decay': 0.0, 'max_grad_norm': 1.0, 'seed': 42, 'chat_template': None, 'quantization': None, 'target_modules': 'all-linear', 'merge_adapter': False, 'peft': False, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'model_ref': None, 'dpo_beta': 0.1, 'max_prompt_length': 128, 'max_completion_length': None, 'prompt_text_column': 'autotrain_prompt', 'text_column': 'autotrain_text', 'rejected_text_column': 'autotrain_rejected_text', 'push_to_hub': False, 'username': None, 'token': None, 'unsloth': False}\u001b[0m\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-24 21:22:50\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_default\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mStarting default/generic CLM training...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-24 21:22:50\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m335\u001b[0m - \u001b[1mloading dataset from disk\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-24 21:22:50\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m394\u001b[0m - \u001b[1mTrain data: Dataset({\n",
      "    features: ['autotrain_text', '__index_level_0__', '__index_level_1__', '__index_level_2__', '__index_level_3__', '__index_level_4__', '__index_level_5__', '__index_level_6__', '__index_level_7__', '__index_level_8__', '__index_level_9__', '__index_level_10__', '__index_level_11__', '__index_level_12__', '__index_level_13__', '__index_level_14__'],\n",
      "    num_rows: 1\n",
      "})\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-24 21:22:50\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m395\u001b[0m - \u001b[1mValid data: None\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-24 21:22:50\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m467\u001b[0m - \u001b[1mconfiguring logging steps\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-24 21:22:50\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m480\u001b[0m - \u001b[1mLogging steps: 10\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-24 21:22:50\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_training_args\u001b[0m:\u001b[36m485\u001b[0m - \u001b[1mconfiguring training args\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-24 21:22:50\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_block_size\u001b[0m:\u001b[36m548\u001b[0m - \u001b[1mUsing block size 1024\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-24 21:22:50\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m583\u001b[0m - \u001b[1mCan use unsloth: False\u001b[0m\n",
      "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-07-24 21:22:50\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m625\u001b[0m - \u001b[33m\u001b[1mUnsloth not available, continuing without it...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-24 21:22:50\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m627\u001b[0m - \u001b[1mloading model config...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-24 21:22:50\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m635\u001b[0m - \u001b[1mloading model...\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:10<00:00,  5.28s/it]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-24 21:23:00\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m666\u001b[0m - \u001b[1mmodel dtype: torch.float32\u001b[0m\n",
      "Running tokenizer on train dataset: 100%|█| 1/1 [00:00<00:00, 307.70 examples/s]\n",
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "Grouping texts in chunks of 1024: 100%|██| 1/1 [00:00<00:00, 1489.45 examples/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-24 21:23:00\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_default\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mcreating trainer\u001b[0m\n",
      "\u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[32m2024-07-24 21:23:02\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m120\u001b[0m - \u001b[31m\u001b[1mtrain has failed due to an exception: Traceback (most recent call last):\n",
      "  File \"/home/luthando/miniconda3/envs/llama/lib/python3.12/site-packages/autotrain/trainers/common.py\", line 117, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/luthando/miniconda3/envs/llama/lib/python3.12/site-packages/autotrain/trainers/clm/__main__.py\", line 23, in train\n",
      "    train_default(config)\n",
      "  File \"/home/luthando/miniconda3/envs/llama/lib/python3.12/site-packages/autotrain/trainers/clm/train_clm_default.py\", line 94, in train\n",
      "    trainer = Trainer(\n",
      "              ^^^^^^^^\n",
      "  File \"/home/luthando/miniconda3/envs/llama/lib/python3.12/site-packages/transformers/trainer.py\", line 535, in __init__\n",
      "    self._move_model_to_device(model, args.device)\n",
      "  File \"/home/luthando/miniconda3/envs/llama/lib/python3.12/site-packages/transformers/trainer.py\", line 782, in _move_model_to_device\n",
      "    model = model.to(device)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/luthando/miniconda3/envs/llama/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 2848, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/luthando/miniconda3/envs/llama/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1173, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/luthando/miniconda3/envs/llama/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 779, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/luthando/miniconda3/envs/llama/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 779, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/luthando/miniconda3/envs/llama/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 779, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"/home/luthando/miniconda3/envs/llama/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 804, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/luthando/miniconda3/envs/llama/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1159, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU \n",
      "\u001b[0m\n",
      "\u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[32m2024-07-24 21:23:02\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m121\u001b[0m - \u001b[31m\u001b[1mCUDA out of memory. Tried to allocate 64.00 MiB. GPU \u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-24 21:23:02\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m350\u001b[0m - \u001b[1mJob ID: 12866\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "!autotrain llm \\\n",
    "    --train \\\n",
    "    --model \"llama/Llama-2-7b-hf/\" \\\n",
    "    --project-name \"fine-llama-2\" \\\n",
    "    --log wandb \\\n",
    "    --data-path \"data/\" \\\n",
    "    --text-column text \\\n",
    "    --lr 2e-5 \\\n",
    "    --batch-size 4 \\\n",
    "    --epochs 3 \\\n",
    "    --block-size 1024 \\\n",
    "    --warmup-ratio 0.03 \\\n",
    "    --lora-r 16 \\\n",
    "    --lora-alpha 32 \\\n",
    "    --lora-dropout 0.05 \\\n",
    "    --weight-decay 0.0 \\\n",
    "    --gradient-accumulation 4 \\\n",
    "    --logging_steps 10 \\\n",
    "    # --fp16 \\\n",
    "    --use-peft \\\n",
    "    # --use-int4 \\\n",
    "    --merge-adapter \\\n",
    "    # --push-to-hub \\\n",
    "    # --token <huggingface-token> \\\n",
    "    # --repo-id <huggingface-repository-address>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
